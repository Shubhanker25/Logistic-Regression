{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Qns-1 What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Ans  Logistic Regression is a supervised learning algorithm used for classification tasks; it models the probability that a given input belongs to a particular class using the logistic (sigmoid) function to output values between 0 and 1. In contrast, Linear Regression is used for predicting continuous numerical values by modeling a linear relationship between input features and the target. The key difference is that Logistic Regression predicts class probabilities (and ultimately categories), while Linear Regression predicts real-valued outcomes."
      ],
      "metadata": {
        "id": "tSuWfNuez9kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-2 What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Ans  The mathematical equation of Logistic Regression is:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "‚à£\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "(\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "ùõΩ\n",
        "2\n",
        "ùë•\n",
        "2\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùõΩ\n",
        "ùëõ\n",
        "ùë•\n",
        "ùëõ\n",
        ")\n",
        "P(y=1‚à£x)=\n",
        "1+e\n",
        "‚àí(Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " x\n",
        "1\n",
        "‚Äã\n",
        " +Œ≤\n",
        "2\n",
        "‚Äã\n",
        " x\n",
        "2\n",
        "‚Äã\n",
        " +‚ãØ+Œ≤\n",
        "n\n",
        "‚Äã\n",
        " x\n",
        "n\n",
        "‚Äã\n",
        " )\n",
        "\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "where\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "‚à£\n",
        "ùë•\n",
        ")\n",
        "P(y=1‚à£x) is the probability of the positive class,\n",
        "ùõΩ\n",
        "0\n",
        "Œ≤\n",
        "0\n",
        "‚Äã\n",
        "  is the intercept,\n",
        "ùõΩ\n",
        "1\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùõΩ\n",
        "ùëõ\n",
        "Œ≤\n",
        "1\n",
        "‚Äã\n",
        " ,‚Ä¶,Œ≤\n",
        "n\n",
        "‚Äã\n",
        "  are the model coefficients, and\n",
        "ùë•\n",
        "1\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùë•\n",
        "ùëõ\n",
        "x\n",
        "1\n",
        "‚Äã\n",
        " ,‚Ä¶,x\n",
        "n\n",
        "‚Äã\n",
        "  are the input features."
      ],
      "metadata": {
        "id": "pHMRruz-0RHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-3 Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Ans  We use the sigmoid function in Logistic Regression because it maps any real-valued input to a range between 0 and 1, making it ideal for modeling probabilities and enabling binary classification by thresholding the output (commonly at 0.5) to decide class labels.\n"
      ],
      "metadata": {
        "id": "CctcDOwf0czc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-4  What is the cost function of Logistic Regression?\n",
        "\n",
        "Ans  The cost function of Logistic Regression, known as log loss or binary cross-entropy, measures how well the model's predicted probabilities align with the actual class labels. It penalizes wrong predictions more heavily as they become more confident and incorrect. The function ensures convexity, allowing optimization algorithms like gradient descent to efficiently find the global minimum, making it suitable for binary classification tasks."
      ],
      "metadata": {
        "id": "tAa6wwe60ksO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-5 What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Ans  Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function, discouraging overly complex models with large coefficients. It is needed because models with too many or too large feature weights may fit the training data well but perform poorly."
      ],
      "metadata": {
        "id": "EwC0IBOk0y6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-6 Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "Ans  Lasso regression uses L1 regularization, which can shrink some coefficients to exactly zero, effectively performing feature selection. Ridge regression uses L2 regularization, which shrinks coefficients but does not eliminate them, keeping all features in the model. Elastic Net combines both L1 and L2 regularization, balancing feature selection and coefficient shrinkage, and is useful when there are correlated or many features."
      ],
      "metadata": {
        "id": "rXkXbNDD06kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-7 When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Ans  Use Elastic Net when you have many correlated features or when you want a balance between feature selection (Lasso) and coefficient shrinkage (Ridge), as it combines both penalties to handle multicollinearity better and select groups of related features more effectively than Lasso or Ridge alone."
      ],
      "metadata": {
        "id": "jnzh6qPN1I86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-8  What is the impact of the regularization parameter (Œª) in Logistic Regression?\n",
        "\n",
        "Ans  The regularization parameter (Œª) controls the strength of the penalty on model complexity in Logistic Regression; a higher Œª increases regularization, shrinking coefficients more to reduce overfitting but possibly causing underfitting, while a lower Œª reduces regularization, allowing the model to fit training data more closely but risking overfitting."
      ],
      "metadata": {
        "id": "mFKuTQjZ1SeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-9 What are the key assumptions of Logistic Regression.\n",
        "\n",
        "Ans  The key assumptions of Logistic Regression are: the dependent variable is binary; there is a linear relationship between the log-odds of the outcome and the independent variables; observations are independent; there is little or no multicollinearity among predictors; and the sample size is sufficiently large for reliable estimates."
      ],
      "metadata": {
        "id": "Wp7sluK51ZSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-10 What are some alternatives to Logistic Regression for classification tasks.\n",
        "\n",
        "Ans  Alternatives to Logistic Regression for classification include Decision Trees, Random Forests, Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), Naive Bayes, Gradient Boosting methods (like XGBoost), and Neural Networks, each offering different strengths depending on data complexity and size.\n"
      ],
      "metadata": {
        "id": "PJA8_PV91nJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-11 What are Classification Evaluation Metrics?\n",
        "\n",
        "Ans  Classification evaluation metrics measure model performance and include accuracy (proportion of correct predictions), precision (correct positive predictions out of all predicted positives), recall or sensitivity (correct positive predictions out of all actual positives), F1-score (harmonic mean of precision and recall), ROC-AUC (area under the receiver operating characteristic curve measuring trade-off between true positive and false positive rates), and confusion matrix (counts of true/false positives and negatives)."
      ],
      "metadata": {
        "id": "iVJGuMEd1vxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-12 How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Ans  Class imbalance in Logistic Regression can cause the model to be biased toward the majority class, leading to poor predictive performance on the minority class because the algorithm tends to minimize overall error and may ignore the less frequent class, resulting in misleadingly high accuracy but low recall or precision for the minority class."
      ],
      "metadata": {
        "id": "cxd5bL_m13H5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-13 What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Ans  Hyperparameter tuning in Logistic Regression involves selecting the best values for parameters like the regularization strength (Œª or C), the type of regularization (L1 or L2), and solver options to optimize model performance, typically done using techniques like grid search or random search with cross-validation."
      ],
      "metadata": {
        "id": "57EvId-x19Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-14 C What are different solvers in Logistic Regression? Which one should be used\n",
        "\n",
        "Ans Logistic Regression solvers include ‚Äòliblinear‚Äô (good for small datasets and supports L1 regularization), ‚Äòlbfgs‚Äô (default, efficient for large datasets, supports L2), ‚Äòsag‚Äô and ‚Äòsaga‚Äô (stochastic methods suitable for very large datasets; ‚Äòsaga‚Äô supports both L1 and L2). Generally, use ‚Äòlbfgs‚Äô for most cases, ‚Äòliblinear‚Äô for small datasets or L1 penalty, and ‚Äòsaga‚Äô for large datasets with sparse data or when using L1 regularization."
      ],
      "metadata": {
        "id": "JmJRhEZr2DgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-15 How is Logistic Regression extended for multiclass classification.\n",
        "\n",
        "Ans Logistic Regression is extended to multiclass classification using strategies like One-vs-Rest (OvR), which trains one binary classifier per class against all others, or Softmax (Multinomial Logistic Regression), which directly models the probability of each class using the softmax function to handle multiple classes simultaneously."
      ],
      "metadata": {
        "id": "1UAEDuDL2KU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-16 What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Ans Logistic Regression is simple, interpretable, computationally efficient, and effective for binary and linearly separable problems, with probabilistic outputs useful for decision-making; however, it assumes a linear relationship between features and log-odds, struggles with complex nonlinear patterns, is sensitive to outliers, and may perform poorly with high-dimensional or highly correlated features without proper regularization.\n"
      ],
      "metadata": {
        "id": "_Z_jkryO2RX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-17 What are some use cases of Logistic Regression?\n",
        "\n",
        "Ans  Logistic Regression is commonly used for binary classification tasks such as spam detection, credit risk assessment, disease diagnosis, customer churn prediction, marketing response modeling, and any scenario requiring probability estimation for two classes."
      ],
      "metadata": {
        "id": "-Sxuo2Jl2dGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-18 What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Ans  Logistic Regression is used for binary classification, modeling the probability of two classes using a sigmoid function, while Softmax Regression (multinomial logistic regression) generalizes this to multiclass problems by using the softmax function to estimate probabilities for three or more classes simultaneously.\n"
      ],
      "metadata": {
        "id": "sJG5MQyE2k_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-19 C How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Ans  Choose One-vs-Rest (OvR) for simpler implementation, faster training on large datasets, and when classes are well-separated, but use Softmax (multinomial logistic regression) when classes are mutually exclusive and you want a single model that jointly optimizes all classes, often leading to better calibrated probabilities and improved performance on complex multiclass problems.\n"
      ],
      "metadata": {
        "id": "Pt9yCYte2rXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns20- C How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Ans  In Logistic Regression, coefficients represent the change in the log-odds of the outcome for a one-unit increase in the predictor; exponentiating a coefficient gives the odds ratio, indicating how much the odds of the positive class multiply with each unit increase in that feature, where values greater than 1 increase odds and less than 1 decrease odds."
      ],
      "metadata": {
        "id": "d5_WpJq823lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-1  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy"
      ],
      "metadata": {
        "id": "uCmh1gSo3AQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0qJb27o3DN2",
        "outputId": "0ca61149-293d-42cb-a06f-eec10da7574e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy?"
      ],
      "metadata": {
        "id": "LYQlQ8pt3Ng1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='saga', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Model accuracy with L1 regularization: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaZm3m-R3RlW",
        "outputId": "73f599d9-b462-48a6-8995-a98701fd871a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L1 regularization: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC"
      ],
      "metadata": {
        "id": "4ssVvMQ93WB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ans\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Model accuracy with L2 regularization: {accuracy:.2f}\")\n",
        "print(\"Model coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiedISw_3axW",
        "outputId": "d245f18a-2004-4aff-9048-738958da863d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L2 regularization: 1.00\n",
            "Model coefficients:\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')?"
      ],
      "metadata": {
        "id": "UrwFKZ5D3g2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Model accuracy with Elastic Net regularization: {accuracy:.2f}\")\n",
        "print(\"Model coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cebBGohR3sYG",
        "outputId": "12cfb8fd-79e7-4657-869d-ed63e8fdc220"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with Elastic Net regularization: 1.00\n",
            "Model coefficients:\n",
            "[[ 0.38656953  1.77585488 -2.4207321  -0.70662401]\n",
            " [ 0.07754859  0.          0.         -0.5816412 ]\n",
            " [-1.25788218 -1.53026918  2.59544966  2.08045153]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-5 Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ],
      "metadata": {
        "id": "sdIM4QdD3ufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Model accuracy with One-vs-Rest multiclass: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4nal5p93xsA",
        "outputId": "d7175c2a-bb3a-4a5e-e7ba-e8c4c4b87dad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with One-vs-Rest multiclass: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy?"
      ],
      "metadata": {
        "id": "4xtY3NCx31l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "predictions = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(f\"Best parameters: {grid.best_params_}\")\n",
        "print(f\"Accuracy with best parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD__-qWt35Zg",
        "outputId": "bfc6d060-b105-4004-945c-7e6bd7018716"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Accuracy with best parameters: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy"
      ],
      "metadata": {
        "id": "PnihQlKp3_7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "print(f\"Average accuracy with Stratified K-Fold: {scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWCSLtJ04Ds4",
        "outputId": "f46b104b-efd3-4efd-84bc-6885267352ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy with Stratified K-Fold: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "SWrpuArj4HRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Replace 'data.csv' with your CSV file path\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Assuming last column is the target\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "K3ikM2zb4JsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "2iaX3lGx4RXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "import numpy as np\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'l1_ratio': [None, 0.1, 0.5, 0.9]  # l1_ratio only used with 'elasticnet'\n",
        "}\n",
        "\n",
        "def filter_params(params):\n",
        "    if params['penalty'] != 'elasticnet':\n",
        "        params['l1_ratio'] = None\n",
        "    return params\n",
        "\n",
        "class FilteredLogisticRegression(LogisticRegression):\n",
        "    def set_params(self, **params):\n",
        "        params = filter_params(params)\n",
        "        return super().set_params(**params)\n",
        "\n",
        "model = FilteredLogisticRegression(max_iter=500)\n",
        "\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = random_search.best_estimator_\n",
        "predictions = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(f\"Best parameters: {random_search.best_params_}\")\n",
        "print(f\"Accuracy with best parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_K4rfCk4Wwd",
        "outputId": "f2c36849-c5f0-49c6-a28b-ef89df568ac1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': np.float64(1.5699452033620265), 'l1_ratio': 0.5, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Accuracy with best parameters: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "20 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.975      0.96666667 0.975      0.325\n",
            " 0.96666667        nan 0.975      0.96666667 0.95833333 0.95833333\n",
            " 0.975      0.96666667 0.975             nan 0.96666667 0.96666667\n",
            " 0.95833333 0.96666667]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy?\n"
      ],
      "metadata": {
        "id": "VosQMU4g4eyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "predictions = ovo_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(f\"One-vs-One Logistic Regression accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFTtcHu34mZH",
        "outputId": "fcc7db88-26fc-4667-eeef-7bddcf71897e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classificationM"
      ],
      "metadata": {
        "id": "cqsTWoOH4oOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "7L_in7Qi4vRm",
        "outputId": "5b10f80d-5f61-4591-de87-a9e9e3b177ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "DTN_LCi44zeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PATpS4x74289",
        "outputId": "9c826943-3dfb-45b6-cc5c-bd055b0180e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.95\n",
            "Recall: 0.99\n",
            "F1-Score: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns 13- Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance"
      ],
      "metadata": {
        "id": "Tua4MRtB48eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uKnnqrk5A-V",
        "outputId": "14e1b257-6d90-4197-b3e1-b92e3eaae500"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.86      0.92       175\n",
            "           1       0.48      0.88      0.62        25\n",
            "\n",
            "    accuracy                           0.86       200\n",
            "   macro avg       0.73      0.87      0.77       200\n",
            "weighted avg       0.92      0.86      0.88       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance?"
      ],
      "metadata": {
        "id": "LDsLWDAt5HF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset (replace 'titanic.csv' with actual path)\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Select features and target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "X = data[features]\n",
        "y = data['Survived']\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X['Age'] = imputer.fit_transform(X[['Age']])\n",
        "\n",
        "# For categorical features, fill missing and encode\n",
        "X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
        "X['Sex'] = LabelEncoder().fit_transform(X['Sex'])\n",
        "X['Embarked'] = LabelEncoder().fit_transform(X['Embarked'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(f\"Titanic Logistic Regression accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "collapsed": true,
        "id": "5Om9TsTZ5KGw",
        "outputId": "9503c4bc-8e61-4429-9d5c-8f3c5e8a28e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-2511808925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load Titanic dataset (replace 'titanic.csv' with actual path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Select features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling"
      ],
      "metadata": {
        "id": "lr636U225gtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "pred_no_scale = model.predict(X_test)\n",
        "acc_no_scale = accuracy_score(y_test, pred_no_scale)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scale:.2f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-puYM3l5jCE",
        "outputId": "a2d11a79-9b3b-4b0c-c741-670e57cdcb5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.00\n",
            "Accuracy with scaling: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score?"
      ],
      "metadata": {
        "id": "-vO750Fa5w3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"ROC-AUC score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9g07wKk50Hw",
        "outputId": "27907818-4f93-460d-af2a-0cc0f37d3967"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy?"
      ],
      "metadata": {
        "id": "1QjetCxW55Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU4h5xT859uW",
        "outputId": "1b530285-88ca-4f1c-bdb5-28a44d6ab3e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-18 Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients?"
      ],
      "metadata": {
        "id": "raBM5hFP6CDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "coef = np.abs(model.coef_).mean(axis=0)  # average coefficients across classes\n",
        "important_features = sorted(zip(feature_names, coef), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Feature importance based on coefficients:\")\n",
        "for feature, importance in important_features:\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBxMINLB6G0Q",
        "outputId": "9c0e0edf-bf86-43d1-fd42-2dec4e01671f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance based on coefficients:\n",
            "petal length (cm): 1.7254\n",
            "petal width (cm): 1.1830\n",
            "sepal width (cm): 0.6417\n",
            "sepal length (cm): 0.3390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns*-19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa\n",
        "ScoreM"
      ],
      "metadata": {
        "id": "KxYxAtSs6LHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Cohen's Kappa Score: {kappa:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdpEW3H56Ops",
        "outputId": "e779e61b-ae27-4ebd-8446-e4bb09666a32"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio"
      ],
      "metadata": {
        "id": "pdFDgXvX6Rdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "Htq5O1Sf6aNh",
        "outputId": "1e312393-3de5-448b-dddf-9bd2bf62dfb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy"
      ],
      "metadata": {
        "id": "cE87W6s46hGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"Accuracy with solver '{solver}': {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhF_fX-66kcz",
        "outputId": "27e98074-0b68-436b-ad6f-b511ac4faf90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with solver 'liblinear': 1.00\n",
            "Accuracy with solver 'saga': 1.00\n",
            "Accuracy with solver 'lbfgs': 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)"
      ],
      "metadata": {
        "id": "oajn3-r461jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhbM-C_l67y3",
        "outputId": "4c18ac1a-f4f3-485a-8ea7-3f32d7839f04"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling"
      ],
      "metadata": {
        "id": "VAgRaL_D6-p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, pred_raw)\n",
        "\n",
        "# Train on standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on raw data: {acc_raw:.2f}\")\n",
        "print(f\"Accuracy on standardized data: {acc_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjVNq8k77DEt",
        "outputId": "f67563f1-4c27-472f-c935-95850c6916d8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 1.00\n",
            "Accuracy on standardized data: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation"
      ],
      "metadata": {
        "id": "flya7ZjX7HDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegressionCV(Cs=10, cv=5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "\n",
        "print(f\"Optimal C: {model.C_[0]:.4f}\")\n",
        "print(f\"Accuracy with optimal C: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtrGPk1s7Jnu",
        "outputId": "ee2bf9bd-d822-4eb7-fbed-ac628b9aa1a2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 0.3594\n",
            "Accuracy with optimal C: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qns-25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "aog10BSo7QR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQSWIlsQ7TcQ",
        "outputId": "dbfe55c6-59f8-4fbc-8115-3e084ac2b054"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
          ]
        }
      ]
    }
  ]
}